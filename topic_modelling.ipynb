{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyN9xGu7YRMyHJCDEmUgsXJX",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ulung3ko/text-anaytics-assignment-2-topic-modelling/blob/main/topic_modelling.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Identifikasi Masalah\n",
        "## Latar Belakang\n",
        "Dalam dunia akademik, artikel ilmiah diterbitkan setiap hari. Volume informasi ini menyulitkan akademisi baik itu mahasiswa, dosen, dan peneliti untuk mengikuti perkembangan terbaru secara menyeluruh. Disinilah penting nya topic modeling, dengan teknik ini, kita dapat secara otomatis mengelompokkan kumpulan abstrak artikel ilmiah ke dalam topik-topik utama seperti Artificial Intelligence, Cybersecurity, atau Computer Vision.\n",
        "\n",
        "Topic modeling memungkinkan kita untuk:\n",
        "\n",
        "- Mengidentifikasi tren penelitian yang sedang berkembang.\n",
        "- Menemukan artikel relevan tanpa membaca satu per satu.\n",
        "- Menyederhanakan eksplorasi literatur dalam jumlah besar.\n",
        "\n",
        "Dalam tugas ini, kita akan membandingkan dua metode topic modeling:\n",
        "\n",
        "- Latent Dirichlet Allocation (LDA).\n",
        "- BERTopic.\n",
        "\n",
        "Fokus tugas ini yaitu menilai model mana yang memberikan hasil pengelompokan topik yang lebih relevan, dapat dimengerti, dan sesuai dengan kategori asli artikel.\n",
        "\n",
        "## Pertanyaan Penelitian\n",
        "1. Topik apa saja yang berhasil ditemukan oleh masing-masing metode?\n",
        "\n",
        "2. Berapa jumlah topik optimal, dan model mana yang memberikan skor kualitas topik terbaik?\n",
        "\n",
        "3. Seberapa sesuai hasil topik dari masing-masing metode dengan label kategori asli dari artikel?\n",
        "\n"
      ],
      "metadata": {
        "id": "91CS8gKP3gj2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Pengumpulan Dataset\n",
        "## Sumber Data\n",
        "Untuk tugas topic modeling ini, dataset yang digunakan adalah dataset publik \"ArXiv Dataset\" yang tersedia di platform Kaggle. Dataset ini berisi metadata dari jutaan publikasi ilmiah dari penerbit arXiv.org.\n",
        "\n",
        "Setiap entri mencakup informasi seperti:\n",
        "- id: ID unik artikel\n",
        "- title: Judul artikel\n",
        "- abstract: Ringkasan isi artikel (digunakan sebagai teks utama analisis)\n",
        "- categories: Label bidang ilmu (misal: cs.AI, cs.CV, math.CO) yang akan digunakan untuk evaluasi model\n",
        "- update_date: Tanggal terakhir artikel diperbarui\n",
        "\n",
        "Dataset ini cocok dengan implementasi topic modeling karena berisi abstrak berkualitas tinggi, terstruktur, dan kaya informasiâ€”ideal untuk pemrosesan bahasa alami (NLP).\n",
        "\n",
        "Link Kaggle : https://www.kaggle.com/datasets/Cornell-University/arxiv\n",
        "\n",
        "## Pengumpulan Data\n",
        "Dataset asli berukuran besar (lebih dari 2 juta entri) dan disimpan dalam format JSON Lines (.jsonl), di mana setiap baris adalah satu objek JSON.\n",
        "\n",
        "Supaya lebih relevan, dataset yang artikel yang digunakan hanya tahun 2021 keatas. Dari tahap ini, diperoleh DataFrame akhir (df_recent) dengan 1.055.586 baris. Karena keterbatasan komputasi, maka diambil sample sebanyak 100.00 saja."
      ],
      "metadata": {
        "id": "d3VBItqi7Fq-"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3eFwjIfP3K_K",
        "outputId": "7f5f5f20-0090-4f06-bed9-4e2417fc9b83"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Path to dataset files: /kaggle/input/arxiv\n"
          ]
        }
      ],
      "source": [
        "# Ini merupakan kode dari kaggle sendiri untuk mendownload dataset nya\n",
        "import kagglehub\n",
        "\n",
        "# Download latest version\n",
        "path = kagglehub.dataset_download(\"Cornell-University/arxiv\")\n",
        "\n",
        "print(\"Path to dataset files:\", path)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"Kode ini digunakan untuk memindahkan file dataset yang telah ter download\n",
        "kedalam direktori '/content/dataset' google colab\"\"\"\n",
        "import shutil\n",
        "# Source folder dari kagglehub\n",
        "src_path = \"/kaggle/input/arxiv\"\n",
        "\n",
        "# Target folder (direktori kerja biasa)\n",
        "dst_path = \"/content/dataset\"\n",
        "\n",
        "# Salin semua isi folder\n",
        "shutil.copytree(src_path, dst_path, dirs_exist_ok=True)\n",
        "\n",
        "print(\"Dataset telah dipindahkan ke:\", dst_path)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ilLaKQAe_PoA",
        "outputId": "e27b5c52-518e-47af-9556-566c2ccdb795"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Dataset telah dipindahkan ke: /content/dataset\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"Kode dibawah digunakan untuk mengkonversi file dataset yang berformat json\n",
        "ke dalam pandas dataframe, dan saat proses nya difilter hanya artikel atau paper\n",
        "dengan tahun >= 2021\"\"\"\n",
        "\n",
        "import pandas as pd\n",
        "import json\n",
        "\n",
        "data = []\n",
        "file_path = '/content/dataset/arxiv-metadata-oai-snapshot.json'\n",
        "start_year = 2021\n",
        "\n",
        "with open(file_path, 'r') as f:\n",
        "    for line in f:\n",
        "        # Megubah setiap baris menjadi dictionary\n",
        "        parsed_line = json.loads(line)\n",
        "\n",
        "        # Membaca tahun dari kolom 'update_date'\n",
        "        # try-except digunakan untuk antisipasi mana tau ada data yang aneh\n",
        "        try:\n",
        "            # Mendapatkan tahun dari kolom 'update date'\n",
        "            year = int(parsed_line['update_date'][:4])\n",
        "\n",
        "            # Simpa data >= 2021\n",
        "            if year >= start_year:\n",
        "                data.append(parsed_line)\n",
        "        except (ValueError, TypeError):\n",
        "            # Jika ada error maka dilanjutkan proses nya\n",
        "            continue\n",
        "\n",
        "# Membuat dataframe dari data-data yang sudah difilter\n",
        "df = pd.DataFrame(data)\n",
        "\n",
        "# Cek hasil dataframe\n",
        "if not df.empty:\n",
        "    print(f\"Berhasil membaca {len(df)} baris data dari tahun {start_year} ke atas.\")\n",
        "    print(\"\\nInformasi DataFrame:\")\n",
        "    df.info()\n",
        "\n",
        "    print(\"\\n5 baris pertama data terbaru:\")\n",
        "    print(df.head())\n",
        "else:\n",
        "    print(f\"Tidak ada data yang ditemukan dari tahun {start_year} ke atas.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RcGNOWYS_qGO",
        "outputId": "68c601bd-3824-4976-a777-e225585f7284"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Berhasil membaca 1055586 baris data dari tahun 2021 ke atas.\n",
            "\n",
            "Informasi DataFrame:\n",
            "<class 'pandas.core.frame.DataFrame'>\n",
            "RangeIndex: 1055586 entries, 0 to 1055585\n",
            "Data columns (total 14 columns):\n",
            " #   Column          Non-Null Count    Dtype \n",
            "---  ------          --------------    ----- \n",
            " 0   id              1055586 non-null  object\n",
            " 1   submitter       1055554 non-null  object\n",
            " 2   authors         1055586 non-null  object\n",
            " 3   title           1055586 non-null  object\n",
            " 4   comments        673326 non-null   object\n",
            " 5   journal-ref     208190 non-null   object\n",
            " 6   doi             317727 non-null   object\n",
            " 7   report-no       25292 non-null    object\n",
            " 8   categories      1055586 non-null  object\n",
            " 9   license         1054272 non-null  object\n",
            " 10  abstract        1055586 non-null  object\n",
            " 11  versions        1055586 non-null  object\n",
            " 12  update_date     1055586 non-null  object\n",
            " 13  authors_parsed  1055586 non-null  object\n",
            "dtypes: object(14)\n",
            "memory usage: 112.7+ MB\n",
            "\n",
            "5 baris pertama data terbaru:\n",
            "          id          submitter  \\\n",
            "0  0704.0033    Maxim A. Yurkin   \n",
            "1  0704.0038    Maxim A. Yurkin   \n",
            "2  0704.0275  George M. Bergman   \n",
            "3  0704.0348  Grazyna Stasinska   \n",
            "4  0704.0479   Thomas Geisser H   \n",
            "\n",
            "                                             authors  \\\n",
            "0  Maxim A. Yurkin, Valeri P. Maltsev, Alfons G. ...   \n",
            "1                Maxim A. Yurkin, Alfons G. Hoekstra   \n",
            "2                   George M. Bergman (U.C.Berkeley)   \n",
            "3                                       G. Stasinska   \n",
            "4                                          T.Geisser   \n",
            "\n",
            "                                               title  \\\n",
            "0  Convergence of the discrete dipole approximati...   \n",
            "1  The discrete dipole approximation: an overview...   \n",
            "2                     Mapping radii of metric spaces   \n",
            "3                   What can emission lines tell us?   \n",
            "4               The affine part of the Picard scheme   \n",
            "\n",
            "                                            comments  \\\n",
            "0  23 pages, 5 figures; added several corrections...   \n",
            "1  36 pages, 1 figure; added several corrections ...   \n",
            "2  24 pages. To appear, Pacific J. Math. Any upda...   \n",
            "3  55 pages, Lectures given at the XVIII Canary I...   \n",
            "4  This is a correct version of the original pape...   \n",
            "\n",
            "                                         journal-ref  \\\n",
            "0  J.Opt.Soc.Am.A 23, 2578-2591 (2006); Erratum: ...   \n",
            "1  J.Quant.Spectrosc.Radiat.Transf. 106, 558-589 ...   \n",
            "2               Pacific J. Math., 236 (2008) 223-261   \n",
            "3                                               None   \n",
            "4                                               None   \n",
            "\n",
            "                                                 doi report-no  \\\n",
            "0    10.1364/JOSAA.23.002578 10.1364/JOSAA.32.002407      None   \n",
            "1  10.1016/j.jqsrt.2007.01.034 10.1016/j.jqsrt.20...      None   \n",
            "2                           10.2140/pjm.2008.236.223      None   \n",
            "3                       10.1017/CBO9780511552038.003      None   \n",
            "4                                               None      None   \n",
            "\n",
            "                       categories  \\\n",
            "0  physics.optics physics.comp-ph   \n",
            "1  physics.optics physics.comp-ph   \n",
            "2                         math.MG   \n",
            "3                        astro-ph   \n",
            "4                 math.AG math.KT   \n",
            "\n",
            "                                             license  \\\n",
            "0  http://creativecommons.org/licenses/by-nc-nd/4.0/   \n",
            "1  http://creativecommons.org/licenses/by-nc-nd/4.0/   \n",
            "2                                               None   \n",
            "3                                               None   \n",
            "4  http://arxiv.org/licenses/nonexclusive-distrib...   \n",
            "\n",
            "                                            abstract  \\\n",
            "0    We performed a rigorous theoretical converge...   \n",
            "1    We present a review of the discrete dipole a...   \n",
            "2    It is known that every closed curve of lengt...   \n",
            "3    1 Generalities\\n  2 Empirical diagnostics ba...   \n",
            "4    We describe the maximal torus and maximal un...   \n",
            "\n",
            "                                            versions update_date  \\\n",
            "0  [{'version': 'v1', 'created': 'Sat, 31 Mar 200...  2022-03-31   \n",
            "1  [{'version': 'v1', 'created': 'Sat, 31 Mar 200...  2022-03-30   \n",
            "2  [{'version': 'v1', 'created': 'Mon, 2 Apr 2007...  2021-10-15   \n",
            "3  [{'version': 'v1', 'created': 'Tue, 3 Apr 2007...  2023-06-07   \n",
            "4  [{'version': 'v1', 'created': 'Wed, 4 Apr 2007...  2021-01-29   \n",
            "\n",
            "                                      authors_parsed  \n",
            "0  [[Yurkin, Maxim A., ], [Maltsev, Valeri P., ],...  \n",
            "1    [[Yurkin, Maxim A., ], [Hoekstra, Alfons G., ]]  \n",
            "2             [[Bergman, George M., , U.C.Berkeley]]  \n",
            "3                                [[Stasinska, G., ]]  \n",
            "4                                  [[Geisser, T., ]]  \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"Kode pada cell ini berfungsi untuk mengambil 100.000 data dari dataset\n",
        "Hal ini dilakukan karena dataset terlalu besar dan tidak cukup komputasi.\"\"\"\n",
        "\n",
        "sample_size = 100000\n",
        "\n",
        "print(f\"\\nUkuran DataFrame asli: {len(df)} baris.\")\n",
        "\n",
        "# Menyimpan 100.000 data ke df_sampled\n",
        "df_sampled = df.sample(n=sample_size, random_state=42)\n",
        "\n",
        "print(f\"Ukuran DataFrame setelah di-sample: {len(df_sampled)} baris.\")\n",
        "\n",
        "# Timpa df asli dengan df_sampled (tidak digunakan lagi)\n",
        "df = df_sampled.copy()\n",
        "\n",
        "# Reset index DataFrame yang sudah dilakukan proses sample\n",
        "df.reset_index(drop=True, inplace=True)\n",
        "\n",
        "# Cek jumlah baris\n",
        "df.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EcKzP5j0p1MW",
        "outputId": "592891a5-243f-4ecf-efea-1626cb2d52ca"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Ukuran DataFrame asli: 1055586 baris.\n",
            "Ukuran DataFrame setelah di-sample: 100000 baris.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(100000, 14)"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Pra-pemrosesan Teks"
      ],
      "metadata": {
        "id": "HNTBTIKZEz2w"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Kode dibawah adalah mendownload beberapa modul dari NLTK untu pre-processing teks\n",
        "import nltk\n",
        "\n",
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')\n",
        "nltk.download('wordnet')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Xw35o3opEt7a",
        "outputId": "b931121e-8352-487f-fef9-2579b431e94d"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "nltk.download('punkt_tab')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BEtvyBKqF3ha",
        "outputId": "8dfe6dcc-d542-4936-8c8a-200ef0807c7f"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt_tab to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt_tab.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"Kode ini digunakan untuk menggabungkan judul artikel dan abstrak menjadi\n",
        "kolom teks untuk memperkaya informasi supaya LDA dan BertTopic lebih bisa\n",
        "menangkap pola dan topik lebih baik\"\"\"\n",
        "\n",
        "df['text'] = df['title'].fillna('') + ' ' + df['abstract'].fillna('')"
      ],
      "metadata": {
        "id": "hBp9FjW9GdLE"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"Kode dibawah didunakan untuk mendefinisikan fungsi pre-processing\n",
        "yang akan diaplikasikan pada dataset\"\"\"\n",
        "\n",
        "import re\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "\n",
        "# Inisialisasi lemmatizer dan stopwords\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "# Menggunakan stopwords berbahasa inggris karena dataset artikel berhasasa inggris\n",
        "stop_words = set(stopwords.words('english'))\n",
        "\n",
        "def preprocess_text(text):\n",
        "    # Case Folding: Mengubah semua teks menjadi huruf kecil\n",
        "    text = text.lower()\n",
        "\n",
        "    # Filtering\n",
        "    text = re.sub(r'http\\S+|www\\S+|https\\S+', '', text, flags=re.MULTILINE)\n",
        "    text = re.sub(r'[^a-z\\s]', '', text)\n",
        "\n",
        "    # Tokenisasi: Memecah teks menjadi token\n",
        "    tokens = word_tokenize(text)\n",
        "\n",
        "    # Stopword Removal & Lemmatization & Filtering kata pendek\n",
        "    cleaned_tokens = []\n",
        "    for word in tokens:\n",
        "        # Cek apakah kata bukan stopword dan panjangnya lebih dari 2 huruf\n",
        "        if word not in stop_words and len(word) > 2:\n",
        "            # Lemmatization: Mengubah kata ke bentuk dasarnya\n",
        "            cleaned_tokens.append(lemmatizer.lemmatize(word))\n",
        "\n",
        "    return cleaned_tokens"
      ],
      "metadata": {
        "id": "XODNYPkkFAPs"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Test hasil fungsi pre-processing yang telah dibuat\n",
        "print(\"Hasil uji coba pra-pemrosesan pada 5 baris pertama:\")\n",
        "contoh_hasil = df['text'].head(5).apply(preprocess_text)\n",
        "print(contoh_hasil)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qRzSrNIgFlG6",
        "outputId": "0e700115-2ae1-4847-f59a-2347a365f0cf"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Hasil uji coba pra-pemrosesan pada 5 baris pertama:\n",
            "0    [morphological, computing, logic, underlying, ...\n",
            "1    [sixteen, point, mathbbp, inverse, galois, pro...\n",
            "2    [aibased, aortic, vessel, tree, segmentation, ...\n",
            "3    [pathwise, unique, solution, stochastic, avera...\n",
            "4    [twodimensional, stabilized, discontinuous, ga...\n",
            "Name: text, dtype: object\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Mengaplikasikan fungsi pada seluruh data di dataset dan disimpan pada kolom baru\n",
        "df['processed_text'] = df['text'].apply(preprocess_text)"
      ],
      "metadata": {
        "id": "_KtWdZW0GzR0"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Menampilkan dataset sebelum dan sesudah pre-processing\n",
        "print(df[['text', 'processed_text']].sample(5))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GS195qMfHBqg",
        "outputId": "2a2f11f0-5f7f-462a-a31e-6cc367813050"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                                                    text  \\\n",
            "13897  The transverse momentum distribution of J/{\\ps...   \n",
            "53899  Distributed Estimation over Directed Graphs Re...   \n",
            "3892   Explainable Face Verification via Feature-Guid...   \n",
            "71605  Quantum to classical parton dynamics in QCD me...   \n",
            "83980  Dual Prompting Image Restoration with Diffusio...   \n",
            "\n",
            "                                          processed_text  \n",
            "13897  [transverse, momentum, distribution, jpsi, mes...  \n",
            "53899  [distributed, estimation, directed, graph, res...  \n",
            "3892   [explainable, face, verification, via, feature...  \n",
            "71605  [quantum, classical, parton, dynamic, qcd, med...  \n",
            "83980  [dual, prompting, image, restoration, diffusio...  \n"
          ]
        }
      ]
    }
  ]
}